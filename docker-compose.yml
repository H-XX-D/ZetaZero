version: '3.8'

services:
  zeta:
    image: ghcr.io/h-xx-d/zetazero:latest
    # Uncomment to build locally instead:
    # build:
    #   context: .
    #   dockerfile: Dockerfile
    container_name: zeta
    restart: unless-stopped
    runtime: nvidia
    network_mode: host
    environment:
      - ZETA_HOST=0.0.0.0
      - ZETA_PORT=8080
      # 14B + 7B Production Models
      - MODEL_MAIN=/models/qwen2.5-14b-instruct-q4_k_m.gguf
      - MODEL_CODER=/models/qwen2.5-coder-7b-instruct-q4_k_m.gguf
      - MODEL_EMBED=/models/nomic-embed-text-v1.5.f16.gguf
      # GPU layers (adjust for your VRAM)
      - GPU_LAYERS_MAIN=49
      - GPU_LAYERS_CODER=35
      # Dream system
      - ZETA_DREAM_INTERVAL=300
      - ZETA_DREAM_DIR=/storage/dreams
      - ZETA_GRAPH_PATH=/storage/blocks/graph.bin
    volumes:
      # Mount models from host
      - ~/models:/models:ro
      # Persist storage (dreams, graph, memories)
      - ~/.zetazero/storage:/storage
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Smaller config for 8GB VRAM GPUs
  zeta-lite:
    image: ghcr.io/h-xx-d/zetazero:latest
    container_name: zeta-lite
    restart: unless-stopped
    runtime: nvidia
    network_mode: host
    profiles:
      - lite
    environment:
      - ZETA_HOST=0.0.0.0
      - ZETA_PORT=8080
      # 7B + 3B for smaller GPUs
      - MODEL_MAIN=/models/qwen2.5-7b-instruct-q4_k_m.gguf
      - MODEL_CODER=/models/qwen2.5-coder-3b-instruct-q4_k_m.gguf
      - MODEL_EMBED=/models/nomic-embed-text-v1.5.f16.gguf
      - GPU_LAYERS_MAIN=35
      - GPU_LAYERS_CODER=28
      - ZETA_DREAM_INTERVAL=600
      - ZETA_DREAM_DIR=/storage/dreams
      - ZETA_GRAPH_PATH=/storage/blocks/graph.bin
    volumes:
      - ~/models:/models:ro
      - ~/.zetazero/storage:/storage
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
