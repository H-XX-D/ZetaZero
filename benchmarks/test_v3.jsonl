{"instance_id": "astropy__astropy-12907", "model_name_or_path": "zeta-14b-7b-4b", "model_patch": "diff --git a/astropy/modeling/tests/test_separable.py b/astropy/modeling/tests/test_separable.py\n--- a/astropy/modeling/tests/test_separable.py\n+++ b/astropy/modeling/tests/test_separable.py\n@@ -1,193 +1,3 @@\n-# Licensed under a 3-clause BSD style license - see LICENSE.rst\n-\"\"\"\n-Test separability of models.\n+The issue reported pertains to the `separability_matrix` function within Astropy's modeling module. The core problem arises when dealing with nested CompoundModels, where the current implementation fails to accurately determine separability between components of such models. In essence, if a model is constructed by nesting multiple compound models together, `separability_matrix` does not properly evaluate each component\u2019s independence from others within these complex structures.\n \n-\"\"\"\n-\n-import numpy as np\n-\n-# pylint: disable=invalid-name\n-import pytest\n-from numpy.testing import assert_allclose\n-\n-from astropy.modeling import custom_model, models\n-from astropy.modeling.core import ModelDefinitionError\n-from astropy.modeling.models import Mapping\n-from astropy.modeling.separable import (\n-    _arith_oper,\n-    _cdot,\n-    _coord_matrix,\n-    _cstack,\n-    is_separable,\n-    separability_matrix,\n-)\n-\n-sh1 = models.Shift(1, name=\"shift1\")\n-sh2 = models.Shift(2, name=\"sh2\")\n-scl1 = models.Scale(1, name=\"scl1\")\n-scl2 = models.Scale(2, name=\"scl2\")\n-map1 = Mapping((0, 1, 0, 1), name=\"map1\")\n-map2 = Mapping((0, 0, 1), name=\"map2\")\n-map3 = Mapping((0, 0), name=\"map3\")\n-rot = models.Rotation2D(2, name=\"rotation\")\n-p2 = models.Polynomial2D(1, name=\"p2\")\n-p22 = models.Polynomial2D(2, name=\"p22\")\n-p1 = models.Polynomial1D(1, name=\"p1\")\n-\n-\n-cm_4d_expected = (\n-    np.array([False, False, True, True]),\n-    np.array(\n-        [\n-            [True, True, False, False],\n-            [True, True, False, False],\n-            [False, False, True, False],\n-            [False, False, False, True],\n-        ]\n-    ),\n-)\n-\n-\n-compound_models = {\n-    \"cm1\": (\n-        map3 & sh1 | rot & sh1 | sh1 & sh2 & sh1,\n-        (\n-            np.array([False, False, True]),\n-            np.array([[True, False], [True, False], [False, True]]),\n-        ),\n-    ),\n-    \"cm2\": (\n-        sh1 & sh2 | rot | map1 | p2 & p22,\n-        (np.array([False, False]), np.array([[True, True], [True, True]])),\n-    ),\n-    \"cm3\": (\n-        map2 | rot & scl1,\n-        (\n-            np.array([False, False, True]),\n-            np.array([[True, False], [True, False], [False, True]]),\n-        ),\n-    ),\n-    \"cm4\": (\n-        sh1 & sh2 | map2 | rot & scl1,\n-        (\n-            np.array([False, False, True]),\n-            np.array([[True, False], [True, False], [False, True]]),\n-        ),\n-    ),\n-    \"cm5\": (\n-        map3 | sh1 & sh2 | scl1 & scl2,\n-        (np.array([False, False]), np.array([[True], [True]])),\n-    ),\n-    \"cm7\": (\n-        map2 | p2 & sh1,\n-        (np.array([False, True]), np.array([[True, False], [False, True]])),\n-    ),\n-    \"cm8\": (rot & (sh1 & sh2), cm_4d_expected),\n-    \"cm9\": (rot & sh1 & sh2, cm_4d_expected),\n-    \"cm10\": ((rot & sh1) & sh2, cm_4d_expected),\n-    \"cm11\": (\n-        rot & sh1 & (scl1 & scl2),\n-        (\n-            np.array([False, False, True, True, True]),\n-            np.array(\n-                [\n-                    [True, True, False, False, False],\n-                    [True, True, False, False, False],\n-                    [False, False, True, False, False],\n-                    [False, False, False, True, False],\n-                    [False, False, False, False, True],\n-                ]\n-            ),\n-        ),\n-    ),\n-}\n-\n-\n-def test_coord_matrix():\n-    c = _coord_matrix(p2, \"left\", 2)\n-    assert_allclose(np.array([[1, 1], [0, 0]]), c)\n-    c = _coord_matrix(p2, \"right\", 2)\n-    assert_allclose(np.array([[0, 0], [1, 1]]), c)\n-    c = _coord_matrix(p1, \"left\", 2)\n-    assert_allclose(np.array([[1], [0]]), c)\n-    c = _coord_matrix(p1, \"left\", 1)\n-    assert_allclose(np.array([[1]]), c)\n-    c = _coord_matrix(sh1, \"left\", 2)\n-    assert_allclose(np.array([[1], [0]]), c)\n-    c = _coord_matrix(sh1, \"right\", 2)\n-    assert_allclose(np.array([[0], [1]]), c)\n-    c = _coord_matrix(sh1, \"right\", 3)\n-    assert_allclose(np.array([[0], [0], [1]]), c)\n-    c = _coord_matrix(map3, \"left\", 2)\n-    assert_allclose(np.array([[1], [1]]), c)\n-    c = _coord_matrix(map3, \"left\", 3)\n-    assert_allclose(np.array([[1], [1], [0]]), c)\n-\n-\n-def test_cdot():\n-    result = _cdot(sh1, scl1)\n-    assert_allclose(result, np.array([[1]]))\n-\n-    result = _cdot(rot, p2)\n-    assert_allclose(result, np.array([[2, 2]]))\n-\n-    result = _cdot(rot, rot)\n-    assert_allclose(result, np.array([[2, 2], [2, 2]]))\n-\n-    result = _cdot(Mapping((0, 0)), rot)\n-    assert_allclose(result, np.array([[2], [2]]))\n-\n-    with pytest.raises(\n-        ModelDefinitionError,\n-        match=r\"Models cannot be combined with the \\\"|\\\" operator; .*\",\n-    ):\n-        _cdot(sh1, map1)\n-\n-\n-def test_cstack():\n-    result = _cstack(sh1, scl1)\n-    assert_allclose(result, np.array([[1, 0], [0, 1]]))\n-\n-    result = _cstack(sh1, rot)\n-    assert_allclose(result, np.array([[1, 0, 0], [0, 1, 1], [0, 1, 1]]))\n-    result = _cstack(rot, sh1)\n-    assert_allclose(result, np.array([[1, 1, 0], [1, 1, 0], [0, 0, 1]]))\n-\n-\n-def test_arith_oper():\n-    # Models as inputs\n-    result = _arith_oper(sh1, scl1)\n-    assert_allclose(result, np.array([[1]]))\n-    result = _arith_oper(rot, rot)\n-    assert_allclose(result, np.array([[1, 1], [1, 1]]))\n-\n-    # ndarray\n-    result = _arith_oper(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]]))\n-    assert_allclose(result, np.array([[1, 1], [1, 1]]))\n-\n-    # Error\n-    with pytest.raises(\n-        ModelDefinitionError, match=r\"Unsupported operands for arithmetic operator: .*\"\n-    ):\n-        _arith_oper(sh1, map1)\n-\n-\n-@pytest.mark.parametrize((\"compound_model\", \"result\"), compound_models.values())\n-def test_separable(compound_model, result):\n-    assert_allclose(is_separable(compound_model), result[0])\n-    assert_allclose(separability_matrix(compound_model), result[1])\n-\n-\n-def test_custom_model_separable():\n-    @custom_model\n-    def model_a(x):\n-        return x\n-\n-    assert model_a().separable\n-\n-    @custom_model\n-    def model_c(x, y):\n-        return x + y\n-\n-    assert not model_c().separable\n-    assert np.all(separability_matrix(model_c()) == [True, True])\n+To illustrate this issue more clearly, consider an example involving nested CompoundModels:"}
{"instance_id": "astropy__astropy-14182", "model_name_or_path": "zeta-14b-7b-4b", "model_patch": "diff --git a/astropy/coordinates/spectral_coordinate.py b/astropy/coordinates/spectral_coordinate.py\n--- a/astropy/coordinates/spectral_coordinate.py\n+++ b/astropy/coordinates/spectral_coordinate.py\n@@ -1,207 +1,5 @@\n-import warnings\n-from textwrap import indent\n+### Introduction\n \n-import numpy as np\n+The current issue concerns the functionality of `astropy.coordinates.SpectralCoord` when attempting to generate RestructuredText (reST) output that includes header rows. The goal is for a table generated from a QTable with units attached via astropy.units to correctly display headers and data in reST format.\n \n-import astropy.units as u\n-from astropy.constants import c\n-from astropy.coordinates import (\n-    ICRS,\n-    CartesianDifferential,\n-    CartesianRepresentation,\n-    SkyCoord,\n-)\n-from astropy.coordinates.baseframe import BaseCoordinateFrame, frame_transform_graph\n-from astropy.coordinates.spectral_quantity import SpectralQuantity\n-from astropy.utils.compat import COPY_IF_NEEDED\n-from astropy.utils.exceptions import AstropyUserWarning\n-\n-__all__ = [\"SpectralCoord\"]\n-\n-\n-class NoVelocityWarning(AstropyUserWarning):\n-    pass\n-\n-\n-class NoDistanceWarning(AstropyUserWarning):\n-    pass\n-\n-\n-KMS = u.km / u.s\n-ZERO_VELOCITIES = CartesianDifferential([0, 0, 0] * KMS)\n-\n-# Default distance to use for target when none is provided\n-DEFAULT_DISTANCE = 1e6 * u.kpc\n-\n-# We don't want to run doctests in the docstrings we inherit from Quantity\n-__doctest_skip__ = [\"SpectralCoord.*\"]\n-\n-\n-def _apply_relativistic_doppler_shift(scoord, velocity):\n-    \"\"\"\n-    Given a `SpectralQuantity` and a velocity, return a new `SpectralQuantity`\n-    that is Doppler shifted by this amount.\n-\n-    Note that the Doppler shift applied is the full relativistic one, so\n-    `SpectralQuantity` currently expressed in velocity and not using the\n-    relativistic convention will temporarily be converted to use the\n-    relativistic convention while the shift is applied.\n-\n-    Positive velocities are assumed to redshift the spectral quantity,\n-    while negative velocities blueshift the spectral quantity.\n-    \"\"\"\n-    # NOTE: we deliberately don't keep sub-classes of SpectralQuantity intact\n-    # since we can't guarantee that their metadata would be correct/consistent.\n-    squantity = scoord.view(SpectralQuantity)\n-\n-    beta = (velocity / c).to_value(u.dimensionless_unscaled)\n-    doppler_factor = np.sqrt((1.0 + beta) / (1.0 - beta))\n-\n-    match squantity.unit.physical_type:\n-        case u.physical.length:\n-            return squantity * doppler_factor\n-        case u.physical.frequency | u.physical.energy | u.physical.wavenumber:\n-            return squantity / doppler_factor\n-        case u.physical.velocity:\n-            return (squantity.to(u.Hz) / doppler_factor).to(squantity.unit)\n-    raise RuntimeError(  # pragma: no cover\n-        f\"Unexpected units in velocity shift: {squantity.unit}. This should not\"\n-        \" happen, so please report this in the astropy issue tracker!\"\n-    )\n-\n-\n-def update_differentials_to_match(\n-    original, velocity_reference, preserve_observer_frame=False\n-):\n-    \"\"\"\n-    Given an original coordinate object, update the differentials so that\n-    the final coordinate is at the same location as the original coordinate\n-    but co-moving with the velocity reference object.\n-\n-    If preserve_original_frame is set to True, the resulting object will be in\n-    the frame of the original coordinate, otherwise it will be in the frame of\n-    the velocity reference.\n-    \"\"\"\n-    if not velocity_reference.data.differentials:\n-        raise ValueError(\"Reference frame has no velocities\")\n-\n-    # If the reference has an obstime already defined, we should ignore\n-    # it and stick with the original observer obstime.\n-    if \"obstime\" in velocity_reference.frame_attributes and hasattr(\n-        original, \"obstime\"\n-    ):\n-        velocity_reference = velocity_reference.replicate(obstime=original.obstime)\n-\n-    # We transform both coordinates to ICRS for simplicity and because we know\n-    # it's a simple frame that is not time-dependent (it could be that both\n-    # the original and velocity_reference frame are time-dependent)\n-\n-    original_icrs = original.transform_to(ICRS())\n-    velocity_reference_icrs = velocity_reference.transform_to(ICRS())\n-\n-    differentials = velocity_reference_icrs.data.represent_as(\n-        CartesianRepresentation, CartesianDifferential\n-    ).differentials\n-\n-    data_with_differentials = original_icrs.data.represent_as(\n-        CartesianRepresentation\n-    ).with_differentials(differentials)\n-\n-    final_icrs = original_icrs.realiz\n-\n-... (truncated) ...\n-\n- target_shift = target_shift.to(u.km / u.s, u.doppler_redshift())\n-            if self._observer is None or self._target is None:\n-                return self.replicate(\n-                    value=_apply_relativistic_doppler_shift(self, target_shift),\n-                    radial_velocity=self.radial_velocity + target_shift,\n-                )\n-\n-        if observer_shift is None:\n-            observer_shift = 0 * KMS\n-        else:\n-            observer_shift = u.Quantity(observer_shift)\n-            if observer_shift.unit.physical_type == \"dimensionless\":\n-                observer_shift = observer_shift.to(u.km / u.s, u.doppler_redshift())\n-\n-        target_icrs = self._target.transform_to(ICRS())\n-        observer_icrs = self._observer.transform_to(ICRS())\n-\n-        pos_hat = SpectralCoord._normalized_position_vector(observer_icrs, target_icrs)\n-\n-        target_velocity = _get_velocities(target_icrs) + target_shift * pos_hat\n-        observer_velocity = _get_velocities(observer_icrs) + observer_shift * pos_hat\n-\n-        target_velocity = CartesianDifferential(target_velocity.xyz)\n-        observer_velocity = CartesianDifferential(observer_velocity.xyz)\n-\n-        new_target = target_icrs.realize_frame(\n-            target_icrs.cartesian.with_differentials(target_velocity)\n-        ).transform_to(self._target)\n-\n-        new_observer = observer_icrs.realize_frame(\n-            observer_icrs.cartesian.with_differentials(observer_velocity)\n-        ).transform_to(self._observer)\n-\n-        init_obs_vel = self._calculate_radial_velocity(observer_icrs, target_icrs)\n-        fin_obs_vel = self._calculate_radial_velocity(new_observer, new_target)\n-\n-        new_data = _apply_relativistic_doppler_shift(self, fin_obs_vel - init_obs_vel)\n-\n-        return self.replicate(value=new_data, observer=new_observer, target=new_target)\n-\n-    def to_rest(self):\n-        \"\"\"\n-        Transforms the spectral axis to the rest frame.\n-        \"\"\"\n-        if self.observer is not None and self.target is not None:\n-            return self.with_observer_stationary_relative_to(self.target)\n-\n-        result = _apply_relativistic_doppler_shift(self, -self.radial_velocity)\n-\n-        return self.replicate(value=result, radial_velocity=0.0 * KMS, redshift=None)\n-\n-    def __repr__(self):\n-        prefixstr = \"<\" + self.__class__.__name__ + \" \"\n-\n-        try:\n-            radial_velocity = self.radial_velocity\n-            redshift = self.redshift\n-        except ValueError:\n-            radial_velocity = redshift = \"Undefined\"\n-\n-        repr_items = [f\"{prefixstr}\"]\n-\n-        if self.observer is not None:\n-            observer_repr = indent(repr(self.observer), 14 * \" \").lstrip()\n-            repr_items.append(f\"    observer: {observer_repr}\")\n-\n-        if self.target is not None:\n-            target_repr = indent(repr(self.target), 12 * \" \").lstrip()\n-            repr_items.append(f\"    target: {target_repr}\")\n-\n-        if (\n-            self._observer is not None and self._target is not None\n-        ) or self._radial_velocity is not None:\n-            if self.observer is not None and self.target is not None:\n-                repr_items.append(\"    observer to target (computed from above):\")\n-            else:\n-                repr_items.append(\"    observer to target:\")\n-            repr_items.append(f\"      radial_velocity={radial_velocity}\")\n-            repr_items.append(f\"      redshift={redshift}\")\n-\n-        if self.doppler_rest is not None or self.doppler_convention is not None:\n-            repr_items.append(f\"    doppler_rest={self.doppler_rest}\")\n-            repr_items.append(f\"    doppler_convention={self.doppler_convention}\")\n-\n-        arrstr = np.array2string(self.view(np.ndarray), separator=\", \", prefix=\"  \")\n-\n-        if len(repr_items) == 1:\n-            repr_items[0] += f\"{arrstr}{self._unitstr:s}\"\n-        else:\n-            repr_items[1] = \"   (\" + repr_items[1].lstrip()\n-            repr_items[-1] += \")\"\n-            repr_items.append(f\"  {arrstr}{self._unitstr:s}\")\n-\n-        return \"\\n\".join(repr_items) + \">\"\n+In Python, using Astropy's `QTable`, one would expect the following code snippet to produce an appropriate reStructuredText representation of tabular data:"}
{"instance_id": "astropy__astropy-14365", "model_name_or_path": "zeta-14b-7b-4b", "model_patch": "diff --git a/docs/conf.py b/docs/conf.py\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -1,214 +1,26 @@\n-# Licensed under a 3-clause BSD style license - see LICENSE.rst\n-#\n-# Astropy documentation build configuration file.\n-#\n-# This file is execfile()d with the current directory set to its containing dir.\n-#\n-# Note that not all possible configuration values are present in this file.\n-#\n-# All configuration values have a default. Some values are defined in\n-# the global Astropy configuration which is loaded here before anything else.\n+SECTION 1: Introduction - Identify bug in docs/conf.py related to QDP commands.\n \n-# If extensions (or modules to document with autodoc) are in another directory,\n-# add these directories to sys.path here. If the directory is relative to the\n-# documentation root, use os.path.abspath to make it absolute, like shown here.\n-# sys.path.insert(0, os.path.abspath('..'))\n-# IMPORTANT: the above commented section was generated by sphinx-quickstart, but\n-# is *NOT* appropriate for astropy or Astropy affiliated packages. It is left\n-# commented out with this explanation to make it clear why this should not be\n-# done. If the sys.path entry above is added, when the astropy.sphinx.conf\n-# import occurs, it will import the *source* version of astropy instead of the\n-# version installed (if invoked as \"make html\" or directly with sphinx), or the\n-# version in the build directory.\n-# Thus, any C-extensions that are needed to build the documentation will *not*\n-# be accessible, and the documentation will not build correctly.\n-# See sphinx_astropy.conf for which values are set there.\n+The issue reported pertains specifically to the `ascii.qdp` table format within our documentation configuration file, `docs/conf.py`. The current implementation assumes that all QDP (Quick Data Plot) commands are written in uppercase. However, this assumption may lead to errors when dealing with mixed-case or lowercase command instances in user-generated files.\n \n-import doctest\n-import inspect\n-import operator\n-import os\n-import sys\n-import tomllib\n-import warnings\n-from datetime import UTC, datetime\n-from importlib import metadata\n-from pathlib import Path\n+Upon reviewing the relevant section of `conf.py`, it is clear there's a discrepancy between how we process and validate these QDP commands against their actual occurrence within data files. For example, if an error directive such as \"ERROR\" appears in a file but is not written in uppercase, our parser will fail to recognize this command correctly due to the case-sensitive nature of Python strings.\n \n-import sphinx\n-from packaging.requirements import Requirement\n-from packaging.specifiers import SpecifierSet\n-from sphinx.util import logging\n+To address this bug effectively, it's crucial that we modify `docs/conf.py` so that all QDP commands are recognized regardless of their letter casing. This change would involve implementing a method or function capable of converting incoming QDP commands into uniform uppercase before processing them further. Such an approach ensures compatibility with various user inputs while maintaining the robustness and flexibility required for accurate command recognition.\n \n-# xref: https://github.com/sphinx-doc/sphinx/issues/13232#issuecomment-2608708175\n-if sys.version_info[:2] >= (3, 13) and sphinx.version_info[:2] < (8, 2):\n-    import pathlib\n+In summary, this bug affects how we interpret QDP directives in `ascii.qdp` files by enforcing case sensitivity unnecessarily, leading to potential parsing errors when dealing with mixed-case or lowercase commands. Resolving it will enhance our documentation's usability and reliability across diverse user scenarios.\n \n-    from sphinx.util.typing import _INVALID_BUILTIN_CLASSES\n+SECTION 2: Analysis - Evaluate impact of enforcing uppercase command requirement\n \n-    _INVALID_BUILTIN_CLASSES[pathlib.Path] = \"pathlib.Path\"\n+The current implementation in `ascii.qdp` imposes a strict condition where all QDP commands must be written in upper case. This enforcement can have significant implications on the parsing process and user experience.\n \n+Firstly, let's consider how this affects data importation accuracy. In scenarios involving large datasets or multiple collaborators working with different conventions, imposing an uppercase-only requirement may introduce unnecessary constraints that hinder workflow efficiency and flexibility. Users might find it cumbersome to adhere strictly to case-sensitive directives if their typical coding practices involve mixed-case notation.\n \n-# from docs import global_substitutions\n+Moreover, the Quick Data Plot (QDP) utility relies heavily on precise command recognition for effective data visualization tasks. Enforcing all commands to be uppercase unnecessarily complicates this process by adding an additional layer of complexity that does not contribute directly to functionality but instead serves as a barrier to entry and usability. This is especially problematic when dealing with automatic script generation tools or software integrations where case sensitivity can lead to unexpected errors if the input files do not conform strictly to the required format.\n \n-logger = logging.getLogger(__name__)\n+To mitigate these issues, we propose modifying `ascii.qdp` so that it treats QDP commands in a case-insensitive manner unless specified otherwise by users through configuration options. This adjustment would make our system more adaptable and user-friendly without compromising its core functionalities or accuracy. It aligns with best practices for software design where flexibility and ease of use are prioritized alongside robustness.\n \n-# -- Check for missing dependencies -------------------------------------------\n-missing_requirements = {}\n-for line in metadata.requires(\"astropy\"):\n-    if 'extra == \"docs\"' in line:\n-        req = Requirement(line.split(\";\")[0])\n-        req_package = req.name.lower()\n-        req_specifier = str(req.specifier)\n+In conclusion, evaluating the impact of enforcing uppercase command requirements highlights potential drawbacks such as reduced usability and increased complexity in data handling processes. By addressing this issue through improved configurability regarding case sensitivity, we can enhance both user satisfaction and system reliability within our documentation framework.\n+SECTION 3: Solution Proposal - Suggest modifications for case-insensitive parsing\n \n-        try:\n-            version = metadata.version(req_package)\n-        except metadata.PackageNotFoundError:\n-            missing_requirements[req_package] = req_specifier\n+To address the identified issue, we propose introducing a configurable option within `ascii.qdp` that allows users to specify whether command recognition should be case-sensitive or not. This change will improve usability and compatibility with various data handling practices employed by different users.\n \n-        if version not in SpecifierSet(req_specifier, prereleases=True):\n-            missing_requirements[req_package] = req_specifier\n-\n-if missing_requirements:\n-    msg = (\n-        \"The following packages could not be found and are required to \"\n-        \"build the documentation:\\n\"\n-        \"%s\"\n-        '\\nPlease install the \"docs\" requirements.',\n-        \"\\n\".join([f\"    * {key} {val}\" for key, val in missing_requirements.items()]),\n-    )\n-    logger.error(msg)\n-    sys.exit(1)\n-\n-from sphinx_astropy.conf.v2 import *  # noqa: E402, F403\n-from sphinx_astropy.conf.v2 import (  # noqa: E402\n-    exclude_patterns,\n-    extensions,\n-    html_theme_options,\n-    intersphinx_mapping,\n-    numpydoc_xref_aliases,\n-    numpydoc_xref_astropy_aliases,\n-    numpydoc_xref_ignore,\n-)\n-\n-# -- Plot configuration -------------------------------------------------------\n-plot_rcparams = {\n-    \"axes.labelsize\": \"large\",\n-    \"figure.figsize\": (6, 6),\n-    \"figure.subplot.hspace\": 0.5,\n-    \"savefig.bbox\": \"tight\",\n-    \"savefig.facecolor\": \"none\",\n-}\n-plot_apply_rcparams = True\n-plot_html_show_source_link = False\n-plot_formats = [\"png\", \"svg\", \"pdf\"]\n-# Don't use the default - which includes a numpy and matplotlib import\n-plot_pre_code = \"\"\n-\n-# -- General configuration ----------------------------------------------------\n-\n-# If your documentation needs a minimal Sphinx version, state it here.\n-needs_sphinx = \"8.2.0\" \n-\n-... (truncated) ...\n-\n-:`~astropy.table.Row`\",\n-    \"Table\": \":class:`~astropy.table.Table`\",\n-    \"QTable\": \":class:`~astropy.table.QTable`\",\n-    # Time\n-    \"Time\": \":class:`~astropy.time.Time`\",\n-    \"TimeDelta\": \":class:`~astropy.time.TimeDelta`\",\n-    # Timeseries\n-    \"TimeSeries\": \":class:`~astropy.timeseries.TimeSeries`\",\n-    \"BinnedTimeSeries\": \":class:`~astropy.timeseries.BinnedTimeSeries`\",\n-    # Distribution\n-    \"Distribution\": \":class:`~astropy.uncertainty.Distribution`\",\n-    # Units\n-    \"PhysicalType\": \":class:`~astropy.units.PhysicalType`\",\n-    \"Quantity\": \":class:`~astropy.units.Quantity`\",\n-    \"Unit\": \":class:`~astropy.units.UnitBase`\",\n-    \"StructuredUnit\": \":class:`~astropy.units.StructuredUnit`\",\n-    # Utils\n-    \"Masked\": \":class:`~astropy.utils.masked.Masked`\",\n-    # Minimum versions\n-    \"minimum_python_version\": f\"{__minimum_python_version__}\",\n-    \"minimum_numpy_version\": f\"{min_versions['numpy']}\",\n-    \"minimum_pyerfa_version\": f\"{min_versions['pyerfa']}\",\n-    \"minimum_matplotlib_version\": f\"{min_versions['matplotlib']}\",\n-    \"minimum_scipy_version\": f\"{min_versions['scipy']}\",\n-    \"minimum_asdf_astropy_version\": f\"{min_versions['asdf-astropy']}\",\n-    \"minimum_packaging_version\": f\"{min_versions['packaging']}\",\n-    \"minimum_pyyaml_version\": f\"{min_versions['pyyaml']}\",\n-    \"minimum_ipython_version\": f\"{min_versions['ipython']}\",\n-    \"minimum_pyarrow_version\": f\"{min_versions['pyarrow']}\",\n-    \"minimum_fsspec_version\": f\"{min_versions['fsspec']}\",\n-    \"minimum_s3fs_version\": f\"{min_versions['s3fs']}\",\n-}\n-# Because sphinxcontrib-globalsubs does not work for regular reStructuredText\n-# links, we first define the links and then process them into the form\n-# of a reStructuredText external link.\n-\n-links_to_become_substitutions: dict[str, str] = {\n-    # Python\n-    \"Python\": \"https://www.python.org\",\n-    \"PEP8\": \"https://www.python.org/dev/peps/pep-0008\",\n-    # Astropy\n-    \"Astropy mailing list\": \"https://mail.python.org/mailman/listinfo/astropy\",\n-    \"astropy-dev mailing list\": \"https://groups.google.com/group/astropy-dev\",\n-    # NumPy\n-    \"NumPy\": \"https://numpy.org\",\n-    \"numpydoc\": \"https://pypi.org/project/numpydoc\",\n-    # erfa\n-    \"ERFA\": \"https://github.com/liberfa/erfa\",\n-    \"PyERFA\": \"https://pyerfa.readthedocs.io\",\n-    # matplotlib\n-    \"Matplotlib\": \"https://matplotlib.org\",\n-    # sofa\n-    \"SOFA\": \"https://www.iausofa.org\",\n-    # scipy\n-    \"SciPy\": \"https://www.scipy.org\",\n-    # packaging\n-    \"packaging\": \"https://packaging.pypa.io\",\n-    # IPython\n-    \"IPython\": \"https://ipython.org\",\n-    # pip\n-    \"pip\": \"https://pip.pypa.io\",\n-    # pipenv\n-    \"pipenv\": \"https://pipenv.pypa.io/en/latest\",\n-    # virtualenv\n-    \"virtualenv\": \"https://pypi.org/project/virtualenv\",\n-    \"virtualenvwrapper\": \"https://pypi.org/project/virtualenvwrapper\",\n-    # conda\n-    \"conda\": \"https://conda.io/docs\",\n-    \"miniconda\": \"https://docs.conda.io/en/latest/miniconda.html\",\n-    # pytest\n-    \"pytest\": \"https://pytest.org/en/latest/index.html\",\n-    \"pytest-astropy\": \"https://github.com/astropy/pytest-astropy\",\n-    \"pytest-doctestplus\": \"https://github.com/astropy/pytest-doctestplus\",\n-    \"pytest-remotedata\": \"https://github.com/astropy/pytest-remotedata\",\n-    # fsspec\n-    \"fsspec\": \"https://filesystem-spec.readthedocs.io\",\n-    # s3fs\n-    \"s3fs\": \"https://s3fs.readthedocs.io\",\n-    # TOPCAT\n-    \"STIL\": \"https://www.star.bristol.ac.uk/mbt/stil\",\n-    \"STILTS\": \"https://www.star.bristol.ac.uk/mbt/stilts\",\n-    \"TOPCAT\": \"https://www.star.bristol.ac.uk/mbt/topcat\",\n-    # OpenAstronomy\n-    \"OpenAstronomy Packaging Guide\": \"https://packaging-guide.openastronomy.org/en/latest\",\n-    # Miscellaneous\n-    \"HDF5\": \"https://www.hdfgroup.org/HDF5\",\n-    \"h5py\": \"https://www.h5py.org\",\n-    \"Parquet\": \"https://parquet.apache.org\",\n-}\n-\n-processed_links = {\n-    key: f\"`{key} <{value}>`_\" for key, value in links_to_become_substitutions.items()\n-}\n-\n-global_substitutions |= processed_links\n-\n-\n-def setup(app):\n-    # Generate the page from Jinja template\n-    app.connect(\"source-read\", rstjinja)\n+Firstly, let's consider adding an optional parameter in the configuration file named 'case_sensitive_commands', defaulted to True for backward compatibility. When set to False, this setting would enable a more flexible parsing mechanism that disregards case when interpreting QDP commands:"}
