# =============================================================================
# Z.E.T.A. Configuration File
# =============================================================================
#
# OVERVIEW
# --------
# This file configures the Z.E.T.A. server and all associated scripts.
# The server reads this file automatically on startup.
#
# SEARCH ORDER (first found wins):
#   1. ./zeta.conf (current directory)
#   2. ~/ZetaZero/zeta.conf (user home)
#   3. /etc/zeta/zeta.conf (system-wide)
#
# FORMAT
# ------
# KEY="value"     # Shell-compatible, quotes optional
# # Comments start with hash
# Empty lines are ignored
#
# USAGE
# -----
#   Start server:   ./zeta-server              # Reads config automatically
#   Override:       ./zeta-server -m /path/to/model.gguf  # CLI takes precedence
#   Test suite:     python scripts/zeta_ultimate_test.py
#
# CURRENT SETUP (Z6 - RTX 5060 Ti 16GB)
# -------------------------------------
#   14B Conscious:  GPU (~8.4GB VRAM)
#   7B Coder:       GPU (~4.7GB VRAM)
#   4B Embedding:   CPU (saves VRAM, dim=2560)
#   Total VRAM:     ~13.2GB / 16GB available
#
# =============================================================================

# -----------------------------------------------------------------------------
# SERVER CONNECTION
# -----------------------------------------------------------------------------
# Where the Z.E.T.A. server runs (for remote deployment, use the remote IP)
ZETA_HOST="192.168.0.165"
ZETA_PORT="8080"
ZETA_URL="http://${ZETA_HOST}:${ZETA_PORT}"

# SSH connection for remote deployment
ZETA_SSH_USER="xx"
ZETA_SSH_HOST="192.168.0.165"

# -----------------------------------------------------------------------------
# MODEL PATHS (on the machine running the server)
# -----------------------------------------------------------------------------
# Primary models - loaded at startup
# The 14B and 7B models load to GPU, 4B embedding loads to CPU to save VRAM
#
# To disable a model, set to empty string: MODEL_14B=""
# The server will skip loading models with empty paths

MODEL_14B="/home/xx/models/qwen2.5-14b-instruct-q4.gguf"           # Main conscious model (GPU)
MODEL_7B_CODER="/home/xx/models/qwen2.5-7b-coder-q4_k_m.gguf"      # Code specialist (GPU)
MODEL_EMBED="/home/xx/models/Qwen3-Embedding-4B-Q4_K_M.gguf"       # Embedding model (CPU, dim=2560)

# Alternative models (for smaller GPUs or different tasks)
MODEL_3B_INSTRUCT="/home/xx/models/qwen2.5-3b-instruct-q4_k_m.gguf"
MODEL_3B_CODER="/home/xx/models/qwen2.5-3b-coder-q4_k_m.gguf"

# -----------------------------------------------------------------------------
# SERVER SETTINGS
# -----------------------------------------------------------------------------
# GPU_LAYERS: Number of layers to offload to GPU
#   999 = all layers (recommended for dedicated GPU)
#   40  = partial offload (for shared VRAM)
#   0   = CPU only
#
# CTX_*: Context window sizes (tokens)
#   Larger = more memory, better long-context performance
#   Smaller = less VRAM usage, faster inference
#
# BATCH_SIZE: Inference batch size
#   Larger = faster throughput, more VRAM
#   2048 is good for 16GB GPU

GPU_LAYERS="999"           # 999 = offload all to GPU (embed uses CPU regardless)
CTX_14B="4096"             # Context size for 14B (4K tokens)
CTX_7B="8192"              # Context size for 7B coder (8K tokens)
CTX_EMBED="512"            # Context size for embedding (small is fine)
BATCH_SIZE="2048"          # Batch size for inference

# -----------------------------------------------------------------------------
# PATHS (on the machine running the server)
# -----------------------------------------------------------------------------
# Build directory where zeta-server binary lives
ZETA_BUILD_DIR="/home/xx/ZetaZero/llama.cpp/build"
ZETA_SERVER_BIN="${ZETA_BUILD_DIR}/bin/zeta-server"

# Persistent storage for HoloGit memory
ZETA_STORAGE="/mnt/HoloGit/blocks"

# Log file
ZETA_LOG="/tmp/zeta.log"

# -----------------------------------------------------------------------------
# TEST SETTINGS
# -----------------------------------------------------------------------------
ZETA_PASSWORD="zeta1234"   # Memory protection password
ZETA_TIMEOUT="180"         # Request timeout in seconds

# =============================================================================
# PROFILE PRESETS
# =============================================================================
# Uncomment a preset to switch configurations. The server reads these in order,
# so later values override earlier ones.
#
# To switch profiles:
#   1. Comment out the current profile section above
#   2. Uncomment one of the presets below
#   3. Restart the server

# -----------------------------------------------------------------------------
# PRESET: Z6 (RTX 5060 Ti 16GB) - CURRENTLY ACTIVE
# -----------------------------------------------------------------------------
# This is the default configuration above. 14B + 7B on GPU, 4B embed on CPU.
# Total VRAM: ~13.2GB

# -----------------------------------------------------------------------------
# PRESET: Orin Nano (8GB Jetson)
# -----------------------------------------------------------------------------
# Uncomment below for 8GB devices - skips 14B, uses 7B + 3B
#
# MODEL_14B=""  # Skip 14B - won't fit
# MODEL_7B_CODER="/home/xx/models/qwen2.5-7b-coder-q4_k_m.gguf"
# MODEL_3B_INSTRUCT="/home/xx/models/qwen2.5-3b-instruct-q4_k_m.gguf"
# GPU_LAYERS="40"
# CTX_14B="2048"
# CTX_7B="2048"

# -----------------------------------------------------------------------------
# PRESET: 12GB GPU (RTX 3060, 4070)
# -----------------------------------------------------------------------------
# Uses 14B with smaller context, skips 7B coder
#
# MODEL_7B_CODER=""  # Skip to save VRAM
# CTX_14B="2048"
# CTX_EMBED="256"

# -----------------------------------------------------------------------------
# PRESET: Local Mac (Apple Silicon)
# -----------------------------------------------------------------------------
# For development/testing on Mac with Metal
#
# ZETA_HOST="localhost"
# MODEL_14B="/Users/hendrixx./models/qwen2.5-14b-instruct-q4.gguf"
# MODEL_7B_CODER="/Users/hendrixx./models/qwen2.5-7b-coder-q4_k_m.gguf"
# MODEL_EMBED="/Users/hendrixx./models/Qwen3-Embedding-4B-Q4_K_M.gguf"
# ZETA_BUILD_DIR="/Users/hendrixx./ZetaZero/llama.cpp/build"
# ZETA_STORAGE="/Users/hendrixx./ZetaZero/storage"
