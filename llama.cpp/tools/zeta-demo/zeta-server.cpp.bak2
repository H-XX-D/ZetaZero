// Z.E.T.A. Server v5.0 - Parallel Dual-Process Engine
// 3B runs PARALLEL to 14B with cyclic correlation feedback

#include "httplib.h"
#include "arg.h"
#include "common.h"
#include "llama.h"
#include "sampling.h"
#include <cstdio>
#include <string>
#include <mutex>
#include <vector>
#include <signal.h>
#include <atomic>
#include "log.h"

extern "C" {
#include "zeta-memory.h"
#include "zeta-integration.h"
#include "zeta-constitution.h"
#include "zeta-dual-process.h"
#include "zeta-embed-integration.h"
#include "zeta-cyclic.h"
#include "zeta-code-mode.h"
}

// 14B Conscious model
static llama_model* g_model_14b = nullptr;
static llama_context* g_ctx_14b = nullptr;

// 3B Subconscious model
static llama_model* g_model_3b = nullptr;

// ZETA contexts
static zeta_context_t* g_zeta = nullptr;
static zeta_dual_ctx_t* g_dual = nullptr;
static zeta_code_ctx_t* g_code = nullptr;  // Code mode context
static llama_model* g_model_3b_coder = nullptr;  // 3B Coder model

static const llama_vocab* g_vocab = nullptr;
static common_params g_params;
static std::mutex g_mutex;
static std::string g_embed_model_path;
static std::string g_storage_dir = "/mnt/HoloGit/blocks";
static int g_n_embd = 0;

// 3B worker thread
static pthread_t g_3b_worker_tid;
static bool g_3b_worker_running = false;

static std::atomic<bool> g_shutdown_requested{false};
static std::atomic<time_t> g_last_activity{0};
static std::thread g_idle_watchdog;

// Tier based on RECENCY (importance affects retrieval, not storage)
static void zeta_apply_temporal_decay(zeta_dual_ctx_t* ctx) {
    if (!ctx) return;
    time_t now = time(NULL);
    for (int i = 0; i < ctx->num_nodes; i++) {
        auto& n = ctx->nodes[i];
        if (!n.is_active) continue;
        float age_secs = (float)(now - n.last_accessed);
        // Tier by recency only - importance is for retrieval ranking
        if (age_secs < 300)        n.current_tier = ZETA_TIER_VRAM;   // < 5 min
        else if (age_secs < 1800)  n.current_tier = ZETA_TIER_RAM;    // < 30 min
        else                       n.current_tier = ZETA_TIER_NVME;   // > 30 min
    }
}



// Idle decay function
// Smart idle decay using Z.E.T.A. functions
static void idle_decay() {
    if (!g_dual) return;
    // Apply temporal decay to all nodes
    zeta_apply_temporal_decay(g_dual);
    // Restage based on decayed salience Ã— current momentum
    // Tier restaging happens automatically during retrieval
    fprintf(stderr, "[IDLE] Applied temporal decay, restaged %d nodes\n", g_dual->num_nodes);
}


// Watchdog thread
static void idle_watchdog_thread() {
    while (!g_shutdown_requested) {
        std::this_thread::sleep_for(std::chrono::seconds(60));
        time_t now = time(NULL);
        time_t idle_secs = now - g_last_activity.load();
        if (idle_secs > 300) {  // 5 min idle
            idle_decay();
        }
    }
}

static httplib::Server* g_server = nullptr;

static void consolidate_memory();
static void signal_handler(int sig);

// Compute momentum from 14B logits (entropy-based)
static float compute_momentum_from_logits(float* logits, int n_vocab) {
    if (!logits || n_vocab <= 0) return 0.5f;
    
    float max_logit = logits[0];
    for (int i = 1; i < n_vocab; i++) {
        if (logits[i] > max_logit) max_logit = logits[i];
    }
    
    float sum_exp = 0;
    for (int i = 0; i < n_vocab; i++) {
        sum_exp += expf(logits[i] - max_logit);
    }
    
    float entropy = 0;
    for (int i = 0; i < n_vocab; i++) {
        float p = expf(logits[i] - max_logit) / sum_exp;
        if (p > 1e-8f) entropy -= p * logf(p);
    }
    
    float momentum = 1.0f - (entropy / 10.0f);
    if (momentum < 0) momentum = 0;
    if (momentum > 1) momentum = 1;
    
    return momentum;
}

static std::string generate(const std::string& prompt, int max_tokens) {
    std::lock_guard<std::mutex> lock(g_mutex);
    
    // === PUSH INPUT TO 3B QUEUE (non-blocking) ===
    zeta_cyclic_push(prompt.c_str(), true, 0.5f);
    
    // === 3B SUBCONSCIOUS: Surface relevant context ===
    zeta_surfaced_context_t surfaced;
    memset(&surfaced, 0, sizeof(surfaced));
    if (g_dual) {
        zeta_surface_context(g_dual, prompt.c_str(), &surfaced);
        if (surfaced.num_nodes > 0) {
            fprintf(stderr, "[3B] Surfaced %d memory nodes for 14B\n", surfaced.num_nodes);
        }
    }
    
    // Augment prompt with surfaced memory
    std::string augmented_prompt = std::string(surfaced.formatted_context) + prompt;
    
    // Tokenize
    std::vector<llama_token> tokens(4096);
    int n_tokens = llama_tokenize(g_vocab, augmented_prompt.c_str(), 
                                   augmented_prompt.length(),
                                   tokens.data(), tokens.size(), true, true);
    if (n_tokens < 0) {
        return "{\"error\": \"tokenization failed\"}";
    }
    tokens.resize(n_tokens);
    
    // Clear KV cache
    llama_memory_t mem = llama_get_memory(g_ctx_14b);
    llama_memory_clear(mem, true);
    
    // Decode prompt
    llama_batch batch = llama_batch_init(4096, 0, 1);
    // Safety: truncate if prompt too long
    if (n_tokens > 3800) {
        fprintf(stderr, "[WARN] Truncating prompt from %d to 3800 tokens\n", n_tokens);
        n_tokens = 3800;
    }
    for (int i = 0; i < n_tokens; i++) {
        common_batch_add(batch, tokens[i], i, {0}, false);
    }
    batch.logits[batch.n_tokens - 1] = true;
    
    if (llama_decode(g_ctx_14b, batch) != 0) {
        llama_batch_free(batch);
        return "{\"error\": \"decode failed\"}";
    }
    
    // Generate with momentum tracking
    std::string output;
    float avg_momentum = 0.0f;
    int n_generated = 0;
    int n_vocab = llama_vocab_n_tokens(g_vocab);
    
    auto* sampler = common_sampler_init(g_model_14b, g_params.sampling);
    
    for (int i = 0; i < max_tokens; i++) {
        float* logits = llama_get_logits_ith(g_ctx_14b, -1);
        
        // Compute momentum from 14B logits
        float momentum = compute_momentum_from_logits(logits, n_vocab);
        avg_momentum += momentum;
        n_generated++;
        
        // Update dual-process momentum
        if (g_dual) {
            zeta_update_momentum(g_dual, momentum);
        }
        
        llama_token tok = common_sampler_sample(sampler, g_ctx_14b, -1);
        common_sampler_accept(sampler, tok, true);
        
        if (llama_vocab_is_eog(g_vocab, tok)) break;
        
        char piece[64] = {0};
        llama_token_to_piece(g_vocab, tok, piece, sizeof(piece), 0, true);
        output += piece;
        
        // Prepare next
        common_batch_clear(batch);
        common_batch_add(batch, tok, n_tokens + i, {0}, true);
        if (llama_decode(g_ctx_14b, batch) != 0) break;
    }
    
    common_sampler_free(sampler);
    llama_batch_free(batch);
    
    avg_momentum = (n_generated > 0) ? (avg_momentum / n_generated) : 0.5f;
    
    // === PUSH OUTPUT TO 3B QUEUE (cyclic feedback) ===
    zeta_cyclic_push(output.c_str(), false, avg_momentum);
    
    // Build response
    char json[8192];
    snprintf(json, sizeof(json),
        "{\"output\": \"%s\", \"tokens\": %d, \"momentum\": %.3f, "
        "\"graph_nodes\": %d, \"graph_edges\": %d}",
        output.c_str(), n_generated, avg_momentum,
        g_dual ? g_dual->num_nodes : 0,
        g_dual ? g_dual->num_edges : 0);
    
    return std::string(json);
}

static void consolidate_memory() {
    if (!g_dual || g_dual->num_nodes == 0) return;
    
    fprintf(stderr, "[CONSOLIDATE] Saving %d nodes, %d edges...\n",
            g_dual->num_nodes, g_dual->num_edges);
    
    char path[1024];
    snprintf(path, sizeof(path), "%s/graph.bin", g_storage_dir.c_str());
    FILE* f = fopen(path, "wb");
    if (f) {
        fwrite(&g_dual->num_nodes, sizeof(int), 1, f);
        fwrite(&g_dual->num_edges, sizeof(int), 1, f);
        fwrite(g_dual->nodes, sizeof(zeta_graph_node_t), g_dual->num_nodes, f);
        fwrite(g_dual->edges, sizeof(zeta_graph_edge_t), g_dual->num_edges, f);
        fclose(f);
        fprintf(stderr, "[CONSOLIDATE] Saved to %s\n", path);
    }
}

static void save_graph() {
    if (!g_dual || g_dual->num_nodes == 0) return;
    
    char path[1024];
    snprintf(path, sizeof(path), "%s/graph.bin", g_storage_dir.c_str());
    FILE* f = fopen(path, "wb");
    if (f) {
        fwrite(&g_dual->num_nodes, sizeof(int), 1, f);
        fwrite(&g_dual->num_edges, sizeof(int), 1, f);
        fwrite(g_dual->nodes, sizeof(zeta_graph_node_t), g_dual->num_nodes, f);
        fwrite(g_dual->edges, sizeof(zeta_graph_edge_t), g_dual->num_edges, f);
        fclose(f);
        fprintf(stderr, "[SAVE] Persisted %d nodes, %d edges to %s\n",
                g_dual->num_nodes, g_dual->num_edges, path);
    } else {
        fprintf(stderr, "[SAVE] ERROR: Could not open %s for writing\n", path);
    }
}

static void load_graph() {
    if (!g_dual) return;
    
    char path[1024];
    snprintf(path, sizeof(path), "%s/graph.bin", g_storage_dir.c_str());
    FILE* f = fopen(path, "rb");
    if (f) {
        size_t r = fread(&g_dual->num_nodes, sizeof(int), 1, f);
        r += fread(&g_dual->num_edges, sizeof(int), 1, f);
        r += fread(g_dual->nodes, sizeof(zeta_graph_node_t), g_dual->num_nodes, f);
        r += fread(g_dual->edges, sizeof(zeta_graph_edge_t), g_dual->num_edges, f);
        fclose(f);
        (void)r;  // Suppress unused warning
        fprintf(stderr, "[LOAD] Restored %d nodes, %d edges from %s\n",
                g_dual->num_nodes, g_dual->num_edges, path);
    }
}

static void signal_handler(int sig) {
    const char* sig_name = (sig == SIGTERM) ? "SIGTERM" : 
                           (sig == SIGINT)  ? "SIGINT"  : "SIGNAL";
    fprintf(stderr, "\n[SHUTDOWN] Received %s...\n", sig_name);
    save_graph();
    g_shutdown_requested = true;
    if (g_server) g_server->stop();
}
// Quiet log callback - filter tensor spam
static void quiet_log_callback(enum ggml_log_level level, const char* text, void* user_data) {
    (void)user_data;
    if (level == GGML_LOG_LEVEL_ERROR || level == GGML_LOG_LEVEL_WARN) {
        fprintf(stderr, "%s", text);
    } else if (level == GGML_LOG_LEVEL_INFO) {
        if (strstr(text, "loading tensor") || strstr(text, "create_tensor") ||
            strstr(text, "llama_kv_cache: layer") || strstr(text, "kv  ")) return;
        fprintf(stderr, "%s", text);
    }
}


int main(int argc, char** argv) {
    // Suppress tensor loading spam
    llama_log_set(quiet_log_callback, NULL);
    if (argc < 3) {
        fprintf(stderr, "Usage: %s -m model_14b.gguf [--model-3b model_3b.gguf] [--model-3b-coder coder.gguf] [--port 9000]\n", argv[0]);
        return 1;
    }
    
    signal(SIGTERM, signal_handler);
    signal(SIGINT, signal_handler);
    
    std::string model_14b_path, model_3b_path, model_3b_coder_path;
    int port = 9000;
    
    g_params.sampling.temp = 0.7f;
    g_params.sampling.top_p = 0.9f;
    g_params.sampling.top_k = 40;
    
    for (int i = 1; i < argc; i++) {
        if (strcmp(argv[i], "-m") == 0 && i+1 < argc) model_14b_path = argv[++i];
        else if (strcmp(argv[i], "--model-3b") == 0 && i+1 < argc) model_3b_path = argv[++i];
        else if (strcmp(argv[i], "--model-3b-coder") == 0 && i+1 < argc) model_3b_coder_path = argv[++i];
        else if (strcmp(argv[i], "--port") == 0 && i+1 < argc) port = atoi(argv[++i]);
        else if (strcmp(argv[i], "--zeta-storage") == 0 && i+1 < argc) g_storage_dir = argv[++i];
    }
    
    fprintf(stderr, "Z.E.T.A. Server v5.0 (Parallel Dual-Process)\n");
    fprintf(stderr, "14B Conscious: %s\n", model_14b_path.c_str());
    fprintf(stderr, "3B Subconscious: %s\n", model_3b_path.empty() ? "(pattern-based)" : model_3b_path.c_str());
    fprintf(stderr, "Port: %d\n", port);
    
    // Load 14B model
    llama_model_params mparams = llama_model_default_params();
    mparams.n_gpu_layers = 99;
    g_model_14b = llama_model_load_from_file(model_14b_path.c_str(), mparams);
    if (!g_model_14b) { fprintf(stderr, "Failed to load 14B model\n"); return 1; }
    
    // Load 3B model if specified
    if (!model_3b_path.empty()) {
        llama_model_params mparams_3b = llama_model_default_params();
        mparams_3b.n_gpu_layers = 99;
        g_model_3b = llama_model_load_from_file(model_3b_path.c_str(), mparams_3b);
        if (g_model_3b) fprintf(stderr, "3B Subconscious model loaded\n");
    }
    
    // Skip 3B Coder at startup - load dynamically on mode switch
    if (false && !model_3b_coder_path.empty()) { // Disabled - dynamic loading
        llama_model_params mparams_coder = llama_model_default_params();
        mparams_coder.n_gpu_layers = 99;
        g_model_3b_coder = llama_model_load_from_file(model_3b_coder_path.c_str(), mparams_coder);
        if (g_model_3b_coder) fprintf(stderr, "3B Coder model loaded (for code mode)\n");
    }
    
    // Init 14B context
    llama_context_params cparams = llama_context_default_params();
    cparams.n_ctx = 4096;
    cparams.n_batch = 4096;
    g_ctx_14b = llama_init_from_model(g_model_14b, cparams);
    if (!g_ctx_14b) { fprintf(stderr, "Failed to create 14B context\n"); return 1; }
    
    g_vocab = llama_model_get_vocab(g_model_14b);
    g_n_embd = llama_model_n_embd(g_model_14b);
    
    // Init ZETA memory
    g_zeta = zeta_context_init(g_ctx_14b, g_storage_dir.c_str(), nullptr, 0.1f, 0.15f, 0.3f, 0.3f);
    
    // Init dual-process engine
    g_dual = zeta_dual_init(g_model_3b ? g_model_3b : g_model_14b, g_storage_dir.c_str());
    // Initialize code mode context (3B Coder not loaded yet - will use 3B Instruct)
    g_code = zeta_code_init(g_dual, g_model_3b, NULL, g_model_14b,
        (g_storage_dir + "/code").c_str());
    if (g_code) fprintf(stderr, "[INIT] Code mode context initialized\n");
    // Set model paths for dynamic swapping
    if (g_code) zeta_set_model_paths(g_code, model_3b_path.c_str(), model_3b_coder_path.c_str());
    if (g_dual) {
        load_graph();  // Restore previous graph
        g_dual->current_session_id = (int64_t)time(NULL);
        fprintf(stderr, "[SESSION] Started session %lld\n", (long long)g_dual->current_session_id);
        fprintf(stderr, "Dual-process engine initialized (nodes=%d, edges=%d)\n",
                g_dual->num_nodes, g_dual->num_edges);
        
        // START 3B PARALLEL WORKER
        g_3b_worker_tid = zeta_3b_start_worker(g_dual);
        g_3b_worker_running = true;
        fprintf(stderr, "3B parallel worker started\n");
    }
    
    httplib::Server svr;
    g_server = &svr;
    
    svr.Post("/generate", [](const httplib::Request& req, httplib::Response& res) {
        g_last_activity = time(NULL);  // Track activity
        std::string prompt = req.get_param_value("prompt");
        int max_tokens = 100;
        if (req.has_param("max_tokens")) max_tokens = std::stoi(req.get_param_value("max_tokens"));
        std::string result = generate(prompt, max_tokens);
        res.set_content(result, "application/json");
    });
    
    svr.Get("/health", [](const httplib::Request&, httplib::Response& res) {
        char json[512];
        snprintf(json, sizeof(json), 
            "{\"status\": \"ok\", \"version\": \"5.0\", "
            "\"parallel_3b\": %s, \"graph_nodes\": %d, \"graph_edges\": %d}",
            g_3b_worker_running ? "true" : "false",
            g_dual ? g_dual->num_nodes : 0,
            g_dual ? g_dual->num_edges : 0);
        res.set_content(json, "application/json");
    });

    // Cache clear endpoint
    svr.Get("/cache/clear", [](const httplib::Request&, httplib::Response& res) {
        if (g_ctx_14b) {
            auto mem = llama_get_memory(g_ctx_14b);
            if (mem) llama_memory_clear(mem, true);
        }
        // Decay based on salience and age - remove lowest 10%
        int removed = 0;
        if (g_dual && g_dual->num_nodes > 10) {
            time_t now = time(NULL);
            for (int i = g_dual->num_nodes - 1; i >= 0 && removed < g_dual->num_nodes / 10; i--) {
                auto& n = g_dual->nodes[i];
                int64_t age = now - n.last_accessed;
                if (n.salience < 0.3f && age > 3600) {  // Low salience + 1hr old
                    n.is_active = false;
                    removed++;
                }
            }
        }
        char resp[128];
        snprintf(resp, sizeof(resp), "{\"status\": \"ok\", \"decayed\": %d}", removed);
        res.set_content(resp, "application/json");
    });

    // Unload 3B to free VRAM
    svr.Get("/system/unload-3b", [](const httplib::Request&, httplib::Response& res) {
        if (g_code && g_code->models.ctx_3b) {
            llama_free(g_code->models.ctx_3b);
            g_code->models.ctx_3b = NULL;
        }
        if (g_code && g_code->models.model_3b_instruct) {
            llama_model_free(g_code->models.model_3b_instruct);
            g_code->models.model_3b_instruct = NULL;
        }
        if (g_code && g_code->models.model_3b_coder) {
            llama_model_free(g_code->models.model_3b_coder);
            g_code->models.model_3b_coder = NULL;
        }
        res.set_content("{\"status\": \"ok\", \"freed\": \"3b_models\"}", "application/json");
    });

    
    svr.Get("/graph", [](const httplib::Request&, httplib::Response& res) {
        if (!g_dual || g_dual->num_nodes == 0) {
            res.set_content("{\"nodes\": [], \"edges\": []}", "application/json");
            return;
        }
        std::string json = "{\"nodes\": [";
        int dumped = 0;
        for (int i = 0; i < g_dual->num_nodes && dumped < 50; i++) {
            auto& n = g_dual->nodes[i];
            if (!n.is_active) continue;  // Only dump active nodes
            if (dumped > 0) json += ",";
            dumped++;
            char buf[512];
            // Sanitize concept_key for JSON output
            char safe_ck[64] = {0};
            for (int j = 0; j < 63 && n.concept_key[j]; j++) {
                if (n.concept_key[j] >= 32 && n.concept_key[j] < 127) safe_ck[j] = n.concept_key[j];
            }
            snprintf(buf, sizeof(buf),
                "{\"id\": %lld, \"label\": \"%s\", \"value\": \"%s\", \"salience\": %.2f, \"concept_key\": \"%s\", \"superseded_by\": %lld}",
                (long long)n.node_id, n.label, n.value, n.salience, safe_ck, (long long)n.superseded_by);
            json += buf;
        }
        json += "], \"edges\": [";
        for (int i = 0; i < g_dual->num_edges && i < 100; i++) {
            if (i > 0) json += ",";
            auto& e = g_dual->edges[i];
            char buf[128];
            snprintf(buf, sizeof(buf),
                "{\"src\": %lld, \"tgt\": %lld, \"type\": %d, \"w\": %.2f}",
                (long long)e.source_id, (long long)e.target_id, e.type, e.weight);
            json += buf;
        }
        json += "]}";
        res.set_content(json, "application/json");
    });
    

    // =========================================================================
    // Project/Code Mode API
    // =========================================================================
    
    svr.Post("/project/open", [](const httplib::Request& req, httplib::Response& res) {
        std::lock_guard<std::mutex> lock(g_mutex);
        
        std::string path = req.get_param_value("path");
        std::string name = req.get_param_value("name");
        std::string desc = req.get_param_value("description");
        
        if (path.empty()) {
            res.set_content("{\"error\": \"path required\"}", "application/json");
            return;
        }
        
        if (!g_code) {
            res.set_content("{\"error\": \"code mode not initialized\"}", "application/json");
            return;
        }
        
        zeta_project_t* proj = zeta_project_open(g_code, path.c_str(),
            name.empty() ? nullptr : name.c_str(),
            desc.empty() ? nullptr : desc.c_str());
        
        if (!proj) {
            res.set_content("{\"error\": \"failed to open project\"}", "application/json");
            return;
        }
        
        char json[2048];
        snprintf(json, sizeof(json),
            "{\"status\": \"ok\", \"project_id\": \"%s\", \"name\": \"%s\", \"mode\": \"code\"}",
            proj->project_id, proj->project_name);
        res.set_content(json, "application/json");
    });
    
    svr.Post("/project/close", [](const httplib::Request&, httplib::Response& res) {
        std::lock_guard<std::mutex> lock(g_mutex);
        
        if (!g_code || !g_code->active_project) {
            res.set_content("{\"error\": \"no active project\"}", "application/json");
            return;
        }
        
        zeta_project_close(g_code);
        res.set_content("{\"status\": \"ok\", \"mode\": \"chat\"}", "application/json");
    });
    
    svr.Get("/project/current", [](const httplib::Request&, httplib::Response& res) {
        std::lock_guard<std::mutex> lock(g_mutex);
        
        if (!g_code) {
            res.set_content("{\"mode\": \"chat\", \"project\": null}", "application/json");
            return;
        }
        
        zeta_project_t* proj = zeta_project_current(g_code);
        if (!proj) {
            res.set_content("{\"mode\": \"chat\", \"project\": null}", "application/json");
            return;
        }
        
        char json[4096];
        snprintf(json, sizeof(json),
            "{\"mode\": \"code\", \"project\": {"
            "\"id\": \"%s\", \"name\": \"%s\", \"path\": \"%s\", "
            "\"description\": \"%s\", \"languages\": \"%s\", "
            "\"tags\": \"%s\", \"status\": \"%s\", "
            "\"file_count\": %d, \"function_count\": %d, \"todo_count\": %d}}",
            proj->project_id, proj->project_name, proj->root_path,
            proj->description, proj->languages, proj->tags, proj->status,
            proj->file_count, proj->function_count, proj->todo_count);
        res.set_content(json, "application/json");
    });
    
    svr.Get("/projects/list", [](const httplib::Request&, httplib::Response& res) {
        std::lock_guard<std::mutex> lock(g_mutex);
        
        if (!g_code) {
            res.set_content("{\"projects\": []}", "application/json");
            return;
        }
        
        std::string json = "{\"projects\": [";
        for (int i = 0; i < g_code->project_count; i++) {
            if (i > 0) json += ",";
            char buf[1024];
            snprintf(buf, sizeof(buf),
                "{\"id\": \"%s\", \"name\": \"%s\", \"status\": \"%s\", \"is_open\": %s}",
                g_code->projects[i].project_id,
                g_code->projects[i].project_name,
                g_code->projects[i].status,
                g_code->projects[i].is_open ? "true" : "false");
            json += buf;
        }
        json += "]}";
        res.set_content(json, "application/json");
    });
    
    svr.Post("/code/check", [](const httplib::Request& req, httplib::Response& res) {
        std::lock_guard<std::mutex> lock(g_mutex);
        
        std::string entity_type = req.get_param_value("type");
        std::string entity_name = req.get_param_value("name");
        std::string file_path = req.get_param_value("file");
        
        if (!g_code || !g_code->active_project) {
            res.set_content("{\"error\": \"no active project\"}", "application/json");
            return;
        }
        
        char reason[512] = {0};
        bool can_create = zeta_can_create(g_code, entity_type.c_str(),
            entity_name.c_str(), file_path.c_str(), reason, sizeof(reason));
        
        char json[1024];
        snprintf(json, sizeof(json),
            "{\"can_create\": %s, \"reason\": \"%s\"}",
            can_create ? "true" : "false", reason);
        res.set_content(json, "application/json");
    });
    
    svr.Get("/code/recent", [](const httplib::Request&, httplib::Response& res) {
        std::lock_guard<std::mutex> lock(g_mutex);
        
        if (!g_code || !g_code->active_project) {
            res.set_content("{\"error\": \"no active project\"}", "application/json");
            return;
        }
        
        char buffer[4096] = {0};
        zeta_surface_recent_work(g_code, buffer, sizeof(buffer));
        
        // Escape for JSON
        std::string escaped;
        for (char c : std::string(buffer)) {
            if (c == '\n') escaped += "\\n";
            else if (c == '\"') escaped += "\\\"";
            else escaped += c;
        }
        
        char json[8192];
        snprintf(json, sizeof(json), "{\"recent_work\": \"%s\"}", escaped.c_str());
        res.set_content(json, "application/json");
    });

    // POST /code/extract - Extract code entities from text
    svr.Post("/code/extract", [](const httplib::Request& req, httplib::Response& res) {
        std::lock_guard<std::mutex> lock(g_mutex);
        
        std::string text = req.get_param_value("text");
        if (text.empty()) {
            res.set_content("{\"error\": \"text required\"}", "application/json");
            return;
        }
        
        if (!g_code || !g_code->active_project) {
            res.set_content("{\"error\": \"no project open\"}", "application/json");
            return;
        }
        
        int added = zeta_code_extract_entities(g_code, text.c_str());
        char json[512];
        snprintf(json, sizeof(json), "{\"status\": \"ok\", \"entities_added\": %d}", added);
        res.set_content(json, "application/json");
    });

    svr.Post("/shutdown", [](const httplib::Request&, httplib::Response& res) {
        res.set_content("{\"status\": \"shutting_down\"}", "application/json");

        save_graph();
    g_shutdown_requested = true;
        if (g_server) g_server->stop();
    });
    
    fprintf(stderr, "\nZ.E.T.A. Server v5.0 listening on port %d\n", port);
    fprintf(stderr, "  POST /generate - Generate with parallel 3B memory\n");
    fprintf(stderr, "  GET  /health   - Health check\n");
    fprintf(stderr, "  GET  /graph    - View memory graph\n");
    fprintf(stderr, "  POST /shutdown - Graceful shutdown\n");
    
    // Start new session (versions old data)
    svr.Post("/session/new", [](const httplib::Request& req, httplib::Response& res) {
        int64_t old_session = g_dual->current_session_id;
        g_dual->current_session_id = (int64_t)time(NULL);
        char buf[256];
        snprintf(buf, sizeof(buf), "{\"status\": \"new_session\", \"old_session\": %lld, \"new_session\": %lld}",
                 (long long)old_session, (long long)g_dual->current_session_id);
        res.set_content(buf, "application/json");
        fprintf(stderr, "[SESSION] New session %lld (old: %lld)\n", 
                (long long)g_dual->current_session_id, (long long)old_session);
    });
    fprintf(stderr, "  POST /project/open  - Open project (code mode)\n");
    fprintf(stderr, "  POST /project/close - Close project (chat mode)\n");
    fprintf(stderr, "  GET  /project/current - Current project info\n");
    fprintf(stderr, "  GET  /projects/list - List all projects\n");
    fprintf(stderr, "  POST /code/check    - Check if can create entity\n");
    fprintf(stderr, "  GET  /code/recent   - Recent work in project\n\n");
    fprintf(stderr, "  POST /code/extract  - Extract code entities from text\n");
    g_last_activity = time(NULL);
    g_idle_watchdog = std::thread(idle_watchdog_thread);
    fprintf(stderr, "[IDLE] Watchdog started (decay@5m, 3B always loaded)\n");
    
    svr.listen("0.0.0.0", port);
    
    fprintf(stderr, "\n[SHUTDOWN] Stopping 3B worker...\n");
    if (g_3b_worker_running) {
        zeta_3b_stop_worker(g_3b_worker_tid);
        g_3b_worker_running = false;
    }
    
    fprintf(stderr, "[SHUTDOWN] Consolidating memory...\n");
    consolidate_memory();
    
    if (g_dual) free(g_dual);
    if (g_zeta) zeta_context_free(g_zeta);
    llama_free(g_ctx_14b);
    llama_model_free(g_model_14b);
    if (g_model_3b) llama_model_free(g_model_3b);
    if (g_model_3b_coder) llama_model_free(g_model_3b_coder);
    
    fprintf(stderr, "[SHUTDOWN] Complete.\n");
    return 0;
}

