Z.E.T.A. Server v5.0 (Parallel Dual-Process)
14B Conscious: /home/xx/models/qwen2.5-7b-coder-q4_k_m.gguf
3B Subconscious: /home/xx/models/qwen2.5-7b-coder-q4_k_m.gguf
Port: 9000
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5060 Ti, compute capability 12.0, VMM: yes
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5060 Ti) (0000:15:00.0) - 15481 MiB free
llama_model_loader: loaded meta data with 38 key-value pairs and 339 tensors from /home/xx/models/qwen2.5-7b-coder-q4_k_m.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_embd_inp       = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   292.36 MiB
load_tensors:        CUDA0 model buffer size =  4168.09 MiB
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5060 Ti) (0000:15:00.0) - 11311 MiB free
llama_model_loader: loaded meta data with 38 key-value pairs and 339 tensors from /home/xx/models/qwen2.5-7b-coder-q4_k_m.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_embd_inp       = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   292.36 MiB
load_tensors:        CUDA0 model buffer size =  4168.09 MiB
3B Subconscious model loaded
[EMBED] Loading embedding model: /home/xx/models/Qwen3-Embedding-4B-Q4_K_M.gguf
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5060 Ti) (0000:15:00.0) - 7141 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 398 tensors from /home/xx/models/Qwen3-Embedding-4B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type q4_K:  216 tensors
llama_model_loader: - type q6_K:   37 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 2.32 GiB (4.95 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2560
print_info: n_embd_inp       = 2560
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 9728
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 3
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 4B
print_info: model params     = 4.02 B
print_info: general.name     = Qwen3 Embedding 4B
print_info: vocab type       = BPE
print_info: n_vocab          = 151665
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151643 '<|endoftext|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   303.74 MiB
load_tensors:        CUDA0 model buffer size =  2375.37 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_seq     = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.59 MiB
llama_kv_cache:      CUDA0 KV buffer size =    72.00 MiB
llama_kv_cache: size =   72.00 MiB (   512 cells,  36 layers,  1/1 seqs), K (f16):   36.00 MiB, V (f16):   36.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   302.23 MiB
llama_context:  CUDA_Host compute buffer size =     6.01 MiB
llama_context: graph nodes  = 1268
llama_context: graph splits = 2
[EMBED] Initialized: dim=2560
Embedding model loaded: /home/xx/models/Qwen3-Embedding-4B-Q4_K_M.gguf
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 8192
llama_context: n_batch       = 4096
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.58 MiB
llama_kv_cache:      CUDA0 KV buffer size =   448.00 MiB
llama_kv_cache: size =  448.00 MiB (  8192 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   311.00 MiB
llama_context:  CUDA_Host compute buffer size =    23.01 MiB
llama_context: graph nodes  = 959
llama_context: graph splits = 2
[CONSTITUTION] Hash: c5e6454b65e7b9c694af9448174f0c54966b32b5fd55b1d01c0b4a0299653e61
[CONSTITUTION] Seed: 0xc6b9e7654b45e6c5

=== Z.E.T.A. Constitutional Lock ===
Path:     (embedded)
Hash:     c5e6454b65e7b9c694af9448174f0c54966b32b5fd55b1d01c0b4a0299653e61
Seed:     0xc6b9e7654b45e6c5
Verified: YES
====================================

[CONSTITUTION] Verified. Entropy key derived successfully.
[MODEL-BIND] Constitutional binding ACTIVE
[MODEL-BIND] Vocabulary size: 152064, Embedding dim: 3584
[ZETA] Compiled without Metal support.
[ZETA] Constitutional lock engaged. Model operational.
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 8192
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.58 MiB
llama_kv_cache:      CUDA0 KV buffer size =   448.00 MiB
llama_kv_cache: size =  448.00 MiB (  8192 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   311.00 MiB
llama_context:  CUDA_Host compute buffer size =    23.01 MiB
llama_context: graph nodes  = 959
llama_context: graph splits = 2
[INIT] Code mode context initialized
[LOAD] Restored 103 nodes, 164 edges from /mnt/HoloGit/blocks/graph.bin (next_id=104)
[SESSION] Started session 1765829013
Dual-process engine initialized (nodes=103, edges=164)
3B parallel worker started

Z.E.T.A. Server v5.0 listening on port 9000
  POST /generate - Generate with parallel 3B memory
  GET  /health   - Health check
  GET  /graph    - View memory graph
  POST /shutdown - Graceful shutdown
  POST /project/open  - Open project (code mode)
  POST /project/close - Close project (chat mode)
  GET  /project/current - Current project info
  GET  /projects/list - List all projects
  POST /code/check    - Check if can create entity
  GET  /code/recent   - Recent work in project

  POST /code/extract  - Extract code entities from text
[3B] Parallel worker started
[IDLE] Watchdog started (decay@5m, 3B always loaded)
[TOOLS] Tool system initialized with 5 tools
[GENERATE] Mode: chat, Project: \n[GENERATE] Received prompt (len=48): Summarize resilience lessons from this chaos run...
[CYCLIC:PUSH] Stored: Summarize resilience lessons from this chaos run...
[CYCLIC:POP] Retrieved: Summarize resilience lessons from this chaos run... is_input=1
[EXTRACT DEBUG] Text starts with: Summarize resilience lessons from this c...
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: identity
[3B-SEMANTIC] Output: resilience_lesson|Chaos Run
[TOK] 3 tokens: Chaos Run...
[3B] Created node: resilience_lesson = Chaos Run (salience=0.85)
[3B] Extracted: resilience_lesson = Chaos Run (concept_key=none)
[3B:WORKER] INPUT: 1 facts extracted
[STREAM] Surfaced: resilience_lesson (id=104, priority=1.02, tokens=11, total=11/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: identity
[STREAM] Surfaced: project_codename (id=103, priority=0.77, tokens=18, total=29/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: identity
[STREAM] Surfaced: project_codename (id=102, priority=0.72, tokens=10, total=39/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: identity
[STREAM] Surfaced: project_codename (id=101, priority=0.60, tokens=12, total=51/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: identity
[STREAM] Surfaced: raw_memory (id=62, priority=0.56, tokens=19, total=70/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: identity
[STREAM] Surfaced: timeout (id=100, priority=0.53, tokens=7, total=77/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: identity
[STREAM] Surfaced: workplace (id=89, priority=0.52, tokens=8, total=85/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: identity
[STREAM] Surfaced: user_name (id=97, priority=0.52, tokens=10, total=95/500)
[FORMAT] Added: Chaos Run (id=104)
[FORMAT] Added: GoKitHTTPTransportWithCircuitBreaker (id=103)
[FORMAT] Added: Pivot (id=102)
[FORMAT] Added: CRDT-Platform (id=101)
[FORMAT] Added: SELECT * FROM graph; DROP TABLE nodes; -- Comment (id=62)
[FORMAT] Added: 30s (id=100)
[FORMAT] Added: Neon (id=89)
[FORMAT] Added: Marcus Chen (id=97)
[STREAM] 8 nodes (95 tokens) surfaced for 14B
[STREAM] Context: [FACTS]
Chaos Run
GoKitHTTPTransportWithCircuitBreaker
Pivot
CRDT-Platform
SELECT * FROM graph; DROP TABLE nodes; -- Comment
30s
Neon
Marcus Chen
[/FACTS]
...
[CYCLIC:PUSH] Stored: , focusing on lessons related to the GoKitHTTPTransportWithC...
[STREAM] Ack served: resilience_lesson (new salience=0.68)
[STREAM] Ack served: project_codename (new salience=0.58)
[STREAM] Ack served: project_codename (new salience=0.46)
[STREAM] Ack served: project_codename (new salience=0.37)
[STREAM] Ack served: raw_memory (new salience=0.05)
[STREAM] Ack served: timeout (new salience=0.35)
[STREAM] Ack served: workplace (new salience=0.35)
[STREAM] Ack served: user_name (new salience=0.33)
[CONFLICT_CHECK] Output: , focusing on lessons related to the GoKitHTTPTransportWithCircuitBreaker and Pi...
[CONFLICT_CHECK] Nodes: 104
[CONFLICT_CHECK] Node 31: age = 34 (sal=0.95)
[CONFLICT_CHECK]   Numeric: 34=num (ctx=unknown)
[CONFLICT_CHECK] Node 58: favorite_X = 42 (sal=0.85)
[CONFLICT_CHECK]   Numeric: 42=num (ctx=unknown)
[CONFLICT_CHECK] Checked 2 nodes, no conflicts
[CONSOLIDATE] Saving 104 nodes, 164 edges...
[CYCLIC:POP] Retrieved: , focusing on lessons related to the GoKitHTTPTransportWithC... is_input=0
[3B:CYCLIC] Edge: Chaos Run <-> Pivot (w=0.93)
[3B:CYCLIC] Edge: Chaos Run <-> GoKitHTTPTransportWithCircuitBreaker (w=0.93)
[3B:WORKER] OUTPUT: 2 correlations
[CONSOLIDATE] Saved to /mnt/HoloGit/blocks/graph.bin
[IDLE] Applied temporal decay, restaged 104 nodes
