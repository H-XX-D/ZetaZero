Z.E.T.A. Server v5.0 (Parallel Dual-Process)
14B Conscious: /home/xx/models/qwen2.5-7b-coder-q4_k_m.gguf
3B Subconscious: /home/xx/models/qwen2.5-7b-coder-q4_k_m.gguf
Port: 9000
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5060 Ti, compute capability 12.0, VMM: yes
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5060 Ti) (0000:15:00.0) - 15483 MiB free
llama_model_loader: loaded meta data with 38 key-value pairs and 339 tensors from /home/xx/models/qwen2.5-7b-coder-q4_k_m.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_embd_inp       = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   292.36 MiB
load_tensors:        CUDA0 model buffer size =  4168.09 MiB
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5060 Ti) (0000:15:00.0) - 11313 MiB free
llama_model_loader: loaded meta data with 38 key-value pairs and 339 tensors from /home/xx/models/qwen2.5-7b-coder-q4_k_m.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_embd_inp       = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   292.36 MiB
load_tensors:        CUDA0 model buffer size =  4168.09 MiB
3B Subconscious model loaded
[EMBED] Loading embedding model: /home/xx/models/Qwen3-Embedding-4B-Q4_K_M.gguf
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5060 Ti) (0000:15:00.0) - 7143 MiB free
llama_model_loader: loaded meta data with 36 key-value pairs and 398 tensors from /home/xx/models/Qwen3-Embedding-4B-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type q4_K:  216 tensors
llama_model_loader: - type q6_K:   37 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 2.32 GiB (4.95 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2560
print_info: n_embd_inp       = 2560
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 9728
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 3
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 4B
print_info: model params     = 4.02 B
print_info: general.name     = Qwen3 Embedding 4B
print_info: vocab type       = BPE
print_info: n_vocab          = 151665
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151643 '<|endoftext|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   303.74 MiB
load_tensors:        CUDA0 model buffer size =  2375.37 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_seq     = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.59 MiB
llama_kv_cache:      CUDA0 KV buffer size =    72.00 MiB
llama_kv_cache: size =   72.00 MiB (   512 cells,  36 layers,  1/1 seqs), K (f16):   36.00 MiB, V (f16):   36.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   302.23 MiB
llama_context:  CUDA_Host compute buffer size =     6.01 MiB
llama_context: graph nodes  = 1268
llama_context: graph splits = 2
[EMBED] Initialized: dim=2560
Embedding model loaded: /home/xx/models/Qwen3-Embedding-4B-Q4_K_M.gguf
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 8192
llama_context: n_batch       = 4096
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.58 MiB
llama_kv_cache:      CUDA0 KV buffer size =   448.00 MiB
llama_kv_cache: size =  448.00 MiB (  8192 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   311.00 MiB
llama_context:  CUDA_Host compute buffer size =    23.01 MiB
llama_context: graph nodes  = 959
llama_context: graph splits = 2
[CONSTITUTION] Hash: c5e6454b65e7b9c694af9448174f0c54966b32b5fd55b1d01c0b4a0299653e61
[CONSTITUTION] Seed: 0xc6b9e7654b45e6c5

=== Z.E.T.A. Constitutional Lock ===
Path:     (embedded)
Hash:     c5e6454b65e7b9c694af9448174f0c54966b32b5fd55b1d01c0b4a0299653e61
Seed:     0xc6b9e7654b45e6c5
Verified: YES
====================================

[CONSTITUTION] Verified. Entropy key derived successfully.
[MODEL-BIND] Constitutional binding ACTIVE
[MODEL-BIND] Vocabulary size: 152064, Embedding dim: 3584
[ZETA] Compiled without Metal support.
[ZETA] Constitutional lock engaged. Model operational.
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 8192
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.58 MiB
llama_kv_cache:      CUDA0 KV buffer size =   448.00 MiB
llama_kv_cache: size =  448.00 MiB (  8192 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   311.00 MiB
llama_context:  CUDA_Host compute buffer size =    23.01 MiB
llama_context: graph nodes  = 959
llama_context: graph splits = 2
[INIT] Code mode context initialized
[LOAD] Restored 95 nodes, 134 edges from /mnt/HoloGit/blocks/graph.bin (next_id=96)
[SESSION] Started session 1765827842
Dual-process engine initialized (nodes=95, edges=134)
3B parallel worker started

Z.E.T.A. Server v5.0 listening on port 9000
  POST /generate - Generate with parallel 3B memory
  GET  /health   - Health check
  GET  /graph    - View memory graph
  POST /shutdown - Graceful shutdown
  POST /project/open  - Open project (code mode)
  POST /project/close - Close project (chat mode)
  GET  /project/current - Current project info
  GET  /projects/list - List all projects
  POST /code/check    - Check if can create entity
  GET  /code/recent   - Recent work in project

  POST /code/extract  - Extract code entities from text
[3B] Parallel worker started
[IDLE] Watchdog started (decay@5m, 3B always loaded)
[TOOLS] Tool system initialized with 5 tools
[GENERATE] Mode: chat, Project: \n[GENERATE] Received prompt (len=112): Outline a distributed rate-limiting strategy for 10K rps mic...
[CYCLIC:PUSH] Stored: Outline a distributed rate-limiting strategy for 10K rps mic...
[CYCLIC:POP] Retrieved: Outline a distributed rate-limiting strategy for 10K rps mic... is_input=1
[EXTRACT DEBUG] Text starts with: Outline a distributed rate-limiting stra...
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: general
[3B-SEMANTIC] Output: rate_limit|10000
[3B] Version update: rate_limit = '4500/min' -> '10000' (v96)
[3B] Extracted: rate_limit = 10000 (concept_key=none)
[3B:WORKER] INPUT: 1 facts extracted
[STREAM] Surfaced: rate_limit (id=96, priority=0.94, tokens=8, total=8/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: general
[STREAM] Surfaced: concurrency_strategy (id=81, priority=0.64, tokens=16, total=24/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: general
[STREAM] Surfaced: library (id=85, priority=0.64, tokens=9, total=33/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: general
[STREAM] Surfaced: raw_memory (id=62, priority=0.63, tokens=19, total=52/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: general
[STREAM] Surfaced: programming_language (id=84, priority=0.63, tokens=11, total=63/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: general
[STREAM] Surfaced: architecture (id=83, priority=0.63, tokens=13, total=76/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: general
[STREAM] Surfaced: secret_code (id=92, priority=0.63, tokens=11, total=87/500)
[STREAM] Query embedded: 1536 dims
[STREAM] Query domain: general
[STREAM] Surfaced: service_topology (id=79, priority=0.62, tokens=14, total=101/500)
[FORMAT] Added: 10000 (id=96)
[FORMAT] Added: deterministic_audit_trail (id=81)
[FORMAT] Added: Apache Arrow (id=85)
[FORMAT] Added: SELECT * FROM graph; DROP TABLE nodes; -- Comment (id=62)
[FORMAT] Added: Rust (id=84)
[FORMAT] Added: air-gapped HPC cluster (id=83)
[FORMAT] Added: tenant_affinity (id=92)
[FORMAT] Added: active-active_failover (id=79)
[STREAM] 8 nodes (101 tokens) surfaced for 14B
[STREAM] Context: [FACTS]
10000
deterministic_audit_trail
Apache Arrow
SELECT * FROM graph; DROP TABLE nodes; -- Comment
Rust
air-gapped HPC cluster
tenant_affinity
active-active_failover
[/FACTS]
...
[CYCLIC:PUSH] Stored: .
<|im_start|>...
[STREAM] Ack served: rate_limit (new salience=0.68)
[STREAM] Ack served: concurrency_strategy (new salience=0.44)
[STREAM] Ack served: library (new salience=0.44)
[STREAM] Ack served: raw_memory (new salience=0.13)
[STREAM] Ack served: programming_language (new salience=0.44)
[STREAM] Ack served: architecture (new salience=0.44)
[STREAM] Ack served: secret_code (new salience=0.44)
[STREAM] Ack served: service_topology (new salience=0.44)
[CONFLICT_CHECK] Output: .
<|im_start|>...
[CONFLICT_CHECK] Nodes: 96
[CONFLICT_CHECK] Node 31: age = 34 (sal=0.95)
[CONFLICT_CHECK]   Numeric: 34=num (ctx=unknown)
[CONFLICT_CHECK] Node 58: favorite_X = 42 (sal=0.85)
[CONFLICT_CHECK]   Numeric: 42=num (ctx=unknown)
[CONFLICT_CHECK] Checked 2 nodes, no conflicts
[CONSOLIDATE] Saving 96 nodes, 135 edges...
[CYCLIC:POP] Retrieved: .
<|im_start|>... is_input=0
[3B:WORKER] OUTPUT: 0 correlations
[CONSOLIDATE] Saved to /mnt/HoloGit/blocks/graph.bin
